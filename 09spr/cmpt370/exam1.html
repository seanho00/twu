<html><head>
<link rel=stylesheet type="text/css" href="/handout.css" />
<link rel=stylesheet type="text/css" href="/exam.css" />
<script type="text/javascript" src="/exam.js"></script>
<style type="text/css"><!--
i { display: none; }
--></style>
<title>CMPT370 Spr2009 Midterm 1, Trinity Western</title>
</head><body>

<div><a href="#" onClick="toggleAnswers()">(answers in web view)</a></div>

<table class=nameblock>
  <tr><td>Name:		</td><td>_______________________________</td></tr>
  <tr><td>Student ID:	</td><td>_______________________________</td></tr>
</table>

<br>Total points: 70

<ol class="main">

<li> Describe <b>Fitt's law</b> in words.  What three quantities does it relate?
<u>[3]</u>
<br><br><i>The <b>time</b> to acquire a target is a (logarithmic) function
  of (the ratio of) the <b>distance</b> and the <b>size</b> of the target.
</i><br>
<br>

<li> The user's mouse cursor is somewhere roughly in the centre third of 
the screen.  List in order the top five <b>easiest locations</b> on the screen
for the user to access using the mouse, and explain why.  (Your answer
of the locations is not as important as your explanation of why.)
<u>[4]</u>
<br><br><i>
  (1) Right under the cursor: no motion required.<br>
  (2) Bottom-right corner of the screen: Fitt's law, corners are like
  infinitely large targets.  Pushing is easier for the arm than pulling;
  the bottom-right corner is an arc away from the body for a right-handed
  user.<br>
  (3) Top-left corner: arc toward the body<br>
  (4) Top-right<br>
  (5) Bottom-left
</i><br>
<br><br><br>

<li> Name at least three of the groundbreaking innovations in 
<b>Engelbart's NLS</b> prototype demonstration in 1968. <u>[3]</u>
<br><br><i>mouse, windowing system, hyperlinks, chording keyboard,
collaborative work (email, IM, video conf)</i><br>
<br>

<li> What was one of the most important advances in the introduction of
the <b>Apple Macintosh</b> in 1984?  (There's room for debate here; argue
your case.)<u>[3]</u>
<br><br><i>Computers for the masses: much cheaper ($3k rather than 
  over $10k), mass-marketing ad campaign including TV spots during
  Superbowl and Olympics.
</i><br><br><br>

<li> What are <b>events</b> and <b>callbacks</b>?
Describe an example. <u>[4]</u>
<br><br><i>
An event is an action, usually triggered by the user: e.g., clicking on a
button, selecting a menu item, pressing Enter, moving the mouse.  A callback
is a procedure/function
invoked when a corresponding event happens.  For example, the exit() function
might be run when the user clicks on a "Quit" button.
</i><br>
<br><br>

<li> The <b>CubeView</b> tutorial example in Lab0 had three C++ files:
CubeView.cxx, CubeMain.cxx, and CubeViewUI.cxx.  Why have three files?
Contrast their purposes. <u>[3]</u>
<br><br><i>
CubeMain was very small, just had the main() function and kicked off an
instance of CubeViewUI.  CubeViewUI was generated by fluid, and just contained
the user interface widgets.  The primary functionality was in CubeView.cxx, in
a class that was used/instantiated by CubeViewUI.
</i><br>
<br><br>

<li> Describe a complete <b>human-computer interface</b> other than our standard
keyboard, monitor, mouse/touchpad/etc.  Describe an application for which this
interface might be better-suited than our traditional interface. <u>[4]</u>
<br><br><br><br><br>

<li> Describe the <b>vonNeumann</b> model of computing. <u>[3]</u>
<br><i>
A <b>computer</b> processes input <b>data</b> according to input
<b>instructions</b>, and produces output <b>results</b>.  
The computer's memory can store both the input data as well as the instructions.
</i><br>
<br><br><br>

<li> Describe <b>Flynn's taxonomy</b> of parallel computing.
(Give more detail than just expanding the acronyms!) <u>[4]</u>
<br><i>
  <b>SISD</b>: single instruction, single data: regular uniprocessing<br>
  <b>SIMD</b>: single instruction, multiple data: apply the same instructions
  to a whole vector of data in parallel.  Also called vector computing.
  Most common form of symmetric multiprocessing.  Early Cray supercomputers
  were mostly SIMD.<br>
  <b>MISD</b>: multiple instruction, single data: applying different operations
  to the same block of data in parallel.  E.g., attempting several decryption
  algorithms to the same ciphertext.  Not commonly used.<br>
  <b>MIMD</b>: multiple instruction, multiple data: several processing nodes,
  each with their own data and own tasks.  Most general form of parallel
  computing.
</i> <br><br>

<li> What does <b>granularity</b> mean in the context of parallel
programming?  What kind of granularity do we generally want, and why?
<u>[4]</u>
<br><br><i>
  Granularity describes the amount of local computation done relative
  to the frequency of communication with other threads.  In distributed
  computing, we generally want coarse granularity, which means a lot of
  computation is done locally with very little communication.
  E.g., FoldingAtHome, a computer can work on a task for weeks before
  reporting back to the server.
</i><br><br><br>

<li> Describe the three parallel programming (<b>API</b>) models we discussed
in class, and name an example API for each.  (The fourth programming model we
talked about is hybrids of these three.) <u>[5]</u>
<br><i><ul>
<li> Threads: master thread forks worker threads; collects results when threads
complete.  OpenMP, POSIX Threads.
<br><br>
<li> Message passing: threads communicate by sending messages to each other.
MPI.
<br><br>
<li> Data parallelism: each thread performs same work in parallel on a
different chunk of the data.  HPF.
</ul></i><br>
<br>

<li> Describe <b>synchronous</b> vs. <b>asynchronous</b> communication 
in general and give an every-day example of each
(need not be computer-related).  <u>[3]</u>
<br><br><i>
  Synchronous: both sender and receiver are connected at the same time.
  Phone call, face-to-face conversation, TCP.<br>
  Asynchronous: sending of message and receiving of message might not
  happen at the same time.  Email, postal mail, voicemail, TV/radio, UDP.
</i><br><br>

<li> Now compare the <b>pros/cons</b> of synchronous vs. asynchronous 
communication in the context of parallel programming. <u>[3]</u>
<br><br><i>
  Synchronous communication is easier to program, but introduces idle time
  while one thread is waiting for another, leading to inefficiencies.<br>
  Asynchronous allows threads to continue while waiting for the other
  side, but logic is needed to tell when the other side has received the
  message.
</i><br><br><br><br><br>

<li> Compare the pros/cons of <b>OpenMP</b> vs. <b>MPI</b>. <u>[4]</u>
<br><br><i>
OpenMP uses the threading model instead of the message-passing model, is easier
to program in and easier to add-on to existing serial code.  The programmer
need not know how many processors the program is actually using.  It is generally
more well-suited to a shared-memory model.  MPI is more complex to program in
but provides more control over synchronization and communication.  MPI often
scales up better to more processors; it is more appropriate for
distributed-memory models.
</i><br>
<br><br><br><br><br>

<li> Describe the (1) <b>shared</b>, (2) <b>distributed</b>, and 
(3) <b>hybrid</b> memory models of
parallel computing.  Draw the diagrams illustrating how memory and processors
are tied together.  What are the advantages/disadvantages of shared memory vs.
distributed memory models? <u>[5]</u>
<br>
<br><br><br><br>
<br><br><br><br>

<li> Describe <b>Amdahl's law</b> of parallel computing.  What is the
take-home message? <u>[4]</u>
<br><br><i>
  Speedup = 1 / ( P/N + (1-P) ), where P is the fraction of code that is
  parallelizable, and N is the number of processors.<br>
  Take-home message is that the important thing is not just the number
  of processors we throw at a problem, but the fraction of code that can
  be parallelized efficiently.
</i><br><br><br><br>

<li> Below is a simple C program to estimate pi by generating random (x,y) 
points in the unit square.  The fraction of points that lie inside the
unit quarter-circle is an estimate of pi/4.  Parallelize this 
program with <b>OpenMP</b> (a complete OpenMP program).
Pay attention to shared/private variables, as well as issues of granularity. 
Use a separate sheet of paper if necessary.  <u>[6]</u>
<pre>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
<i>#include &lt;omp.h&gt;</i>

int main(int argc, char *argv[]) {

    double x, y, pi;
    int i, count, num_steps;

    num_steps = atoi(argv[1]);		/* command-line arg */

<i>#pragma omp parallel for private(x,y) schedule(static, 1000)</i>
        for (i=0; i &lt; num_steps; i++) {

            x = (double) rand()/RAND_MAX;
            y = (double) rand()/RAND_MAX;
            if (x*x + y*y &lt; 1) count++;	/* in unit circle? */

        }

    pi = 4.0 * count / num_steps;
    printf("pi = %32.30f\n", pi);
    return 0;
}</pre>
<br><i>(See <a href="openmp/pi/pi-monte.cpp">pi-monte.cpp</a>)</i>

<li class="break"> Attached is a printout of 
<a href="http://alistapart.com/">A List Apart</a>'s
frontpage (they are a free online magazine publishing articles on
web design and HTML/CSS/JS techniques).
Critique their user interface <b>design</b>: good points and
bad points.  Bear in mind that this is a static printout of a website.
Be as detailed as you can, and cite specific examples.  You may wish to
circle items on the printout.
<u>[5]</u>
<br><br>

</ol>

<br>
<img src="alistapart-09.png" width="100%"/><br>

</body></html>


