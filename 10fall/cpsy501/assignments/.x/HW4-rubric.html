<html><head>
<link rel=stylesheet type="text/css" href="/handout.css" />
<title>HW4 Rubric :: CPSY501 10FA :: TWU</title>
</head><body>

<h1><a href="http://twu.seanho.com/10fall/cpsy501/">
    CPSY 501 Fall 2010</a> HW4 Answer Key / Marking Rubric</h1>

<h2>Total: 20pts</h2>

<h2>APA style (2pts)</h2><ul>
  <li> Format of reporting statistical results (most important!)
  <li> Use appropriate statistical symbols, no typos
  <li> Pagination and running heads, clear organization
  <li> Figures/tables not required, but if present must be in APA style
  <li> Detailed output goes in separate SPSS output file
</ul>

<h2>(1) Methods of Training (6pts)</h2>
The student’s reasoning and rationale for agreeing or disagreeing with
Hill&amp;Lent is more important than what their actual judgment is.
<ul>
  <li> Arguments in favour of omitting the Uhlemann study: <ul>
    <li> Its negative effect size is extremely different
    <li> Statistical "outlier", possibly atypical result
    <li> Possible issues with Uhlemann's methodology with administering the
    modelling condition. 
  </ul>
  <li> Arguments in favour of including the Uhlemann study:<ul>
    <li> It is only a possible problem (Q statistic on homogeneity)
    <li> We don't know for sure that it's atypical, given how few studies (k=14) we have
    total.
    <li> Perhaps the effect sizes really aren't homogeneous, then our
    analysis should reflect that.
    <li> Excluding the outlier is extreme (could perhaps include it with a
    lower weight); it is prejudging the research question.
  </ul>
</ul>
Overall, a conservative strategy would counsel that it is unwise to ignore
completely a negative effect at this early stage in research, with so few other
studies in the meta-analysis.  The result is indeterminate until more and
better research is done.

<h2>(2) Additive Effects of Training (6pts)</h2>
Again, the student's reasoning and rationale for agreeing or disagreeing with
Hill&amp;Lent is more important than what their actual judgment is.<ul>
  <li> Arguments in favour of omitting the Dalton study: <ul>
    <li> Its large effect size influences the d<sub>+</sub>
    weighted-average effect size considerably
    <li> Statistical "outlier", possibly atypical result.
  </ul>
  <li> Arguments in favour of including the Dalton study:<ul>
    <li> It is only a possible problem (Q statistic on homogeneity)
    <li> We don't know for sure that it's atypical, given how few studies we
    have total (k=5 studies, n=201 students).
    <li> Perhaps the effect sizes really aren't homogeneous, then our analysis
    should reflect that.
    <li> There is no theoretical justification given for excluding this study
    (e.g., problems with Dalton's methodology).
    <li> Excluding the outlier is extreme (could perhaps include it with a
    lower weight); it is prejudging the research question.
  </ul>
</ul>
Overall, the effect of ignoring the Dalton study is to lower the
weighted-average effect size d+, which is erring on the side of caution, a
conservatively small effect size.  The result is indeterminate until more and
better research is done.

<h2>(3) Other Comparability (6pts)</h2><ul>
  <li> There is a lot of freedom in possible issues with the article that the
  student could raise: e.g., amount of training time may confound; gender may
  be a moderator, etc.
  <li> Persuasiveness of meta-analyses comes from quality of studies (design,
  methodology, follow-up, etc.), comparability of studies (are they looking at
  the same variables?  Same grouping of participants?  Drawing from similar
  populations?), and potential moderators that may not have been considered.
  <li> Can we clearly separate the “instruction”, “modelling”, and “feedback”
  conditions from one another in training of helping skills?
</ul>

</body></html>
